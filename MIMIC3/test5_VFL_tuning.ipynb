{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn import metrics\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================specify the model to fit============================\n",
    "fitmodel = sm.families.Binomial(link = sm.families.links.logit())\n",
    "#========================fit oracle model========================\n",
    "\n",
    "#=============================function to calculate the inverse logit link============================\n",
    "def invLink(lp):\n",
    "    pv = 1/(1 + math.exp(-lp))\n",
    "    return pv\n",
    "\n",
    "# vectorize the function\n",
    "invLinkVec = np.vectorize(invLink)\n",
    "#========================Define the log-likelihood for evaluation========================\n",
    "# logistic regression log-likelihood\n",
    "def loglikeli(YEval, predCombined):\n",
    "    ll = np.mean(YEval * np.log(predCombined) + (1-YEval) * np.log(1 - predCombined))\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================read data=====================================\n",
    "import pickle\n",
    "fname = \"data_splitted_dic.p\"\n",
    "infile = open(fname, 'rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTrain = new_dict[\"YTrain\"]\n",
    "YVal = new_dict[\"YVal\"]\n",
    "YTest = new_dict[\"YTest\"]\n",
    "\n",
    "XATrain = new_dict[\"XATrain\"]\n",
    "XBTrain = new_dict[\"XBTrain\"]\n",
    "XAVal = new_dict[\"XAVal\"]\n",
    "XBVal = new_dict[\"XBVal\"]\n",
    "XATest = new_dict[\"XATest\"]\n",
    "XBTest = new_dict[\"XBTest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and val for AE-AL\n",
    "\n",
    "Y = np.concatenate((YTrain, YVal), axis = 0)\n",
    "\n",
    "XA = np.concatenate((XATrain, XAVal), axis = 0)\n",
    "\n",
    "XB = np.concatenate((XBTrain, XBVal), axis = 0)\n",
    "\n",
    "X = np.concatenate((XA, XB),axis = 1)\n",
    "XTest = np.concatenate((XATest, XBTest),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit and evaulate the VFL\n",
    "# when batchsize=0, apply batch gradient descent instead of minibatch gradient descent\n",
    "def fitVFL(Model, batchsize, Q, mu, eta0, etaFun, TrainPattern, Y, XA, XB, YEval, XEval, kNum, fitmodel, loglikeli, reportAUC, reportll):\n",
    "    X = np.concatenate((XA, XB),axis = 1)\n",
    "\n",
    "    # X matrix with intercept\n",
    "    Xintercept = sm.add_constant(X)\n",
    "\n",
    "    # XA matrix with intercept\n",
    "    XAintercept = sm.add_constant(XA)\n",
    "\n",
    "    # XB matrix with intercept\n",
    "    XBintercept = sm.add_constant(XB)\n",
    "\n",
    "    pB = XB.shape[1]\n",
    "\n",
    "    XEvalintercept = sm.add_constant(XEval)\n",
    "    # betaOracle: a 1-d np array of oracle model fitted coefficients\n",
    "    # betaTemp: a 1-d np array of assisted learning model fitted coefficients\n",
    "    # Y: predictor data\n",
    "    # predCombined: prediction from the assisted learning model\n",
    "    def eval(betaOracle, betaTemp, Y, predCombined, loglikeli):\n",
    "        # calculate the Euclidean distance between the oracle model and assiste learning model fitted coefficients.\n",
    "        EuDis = distance.euclidean(betaOracle, betaTemp)\n",
    "        \n",
    "        if reportAUC:\n",
    "        #  calculate the AUC\n",
    "            fpr, tpr, _ = metrics.roc_curve(Y, predCombined)\n",
    "            AUC = metrics.auc(fpr, tpr)\n",
    "        if reportll:\n",
    "            # calculate the loglikelihood\n",
    "            ll = loglikeli(Y, predCombined)\n",
    "\n",
    "        if reportAUC:\n",
    "            if reportll:\n",
    "                return EuDis, AUC, ll\n",
    "            else:\n",
    "                return EuDis, AUC\n",
    "        elif reportll:\n",
    "            return EuDis, ll\n",
    "        else:\n",
    "            return EuDis\n",
    "    if Model == \"logcosh\":\n",
    "            #========================define functions for fitting log-cosh====================\n",
    "            logcoshA = 0.3\n",
    "            def gradCal(Y, XAintercept, beta0):\n",
    "                # get the residual\n",
    "                resid = Y - np.dot(XAintercept, beta0)\n",
    "                # calculate the gradient\n",
    "                grad = np.mean((np.tanh(logcoshA * resid).reshape(XAintercept.shape[0],-1)) * XAintercept, axis = 0)\n",
    "                return(grad)\n",
    "            # gradient calculation with offset\n",
    "            def gradCalAssisted(Y, XAintercept, offset, beta0):\n",
    "                # get the residual\n",
    "                resid = Y - np.dot(XAintercept, beta0) - offset\n",
    "                # calculate the gradient\n",
    "                grad = np.mean((np.tanh(logcoshA * resid).reshape(XAintercept.shape[0],-1)) * XAintercept, axis = 0)\n",
    "                return(grad)\n",
    "\n",
    "            def HessCal(Y, XAintercept, beta0):\n",
    "                # get the residual\n",
    "                resid = Y - np.dot(XAintercept, beta0)\n",
    "                # calculate the Hessian\n",
    "                Hess = logcoshA * X.shape[0]**(-1) * np.dot(np.dot(np.transpose(XAintercept), np.diag(np.cosh(logcoshA * resid)**(-1))), XAintercept)\n",
    "                return(Hess)\n",
    "            def fitLogCosh(Y, XAintercept):\n",
    "                # fit a linear regression as the initial value\n",
    "                model = sm.GLM(endog = Y, exog = XAintercept, family = fitmodel)\n",
    "                # get initial value\n",
    "                beta0 = model.fit().params\n",
    "\n",
    "                # evaluate the gradient\n",
    "                grad_updated = gradCal(Y, XAintercept, beta0)\n",
    "                gradL2 = np.mean(grad_updated**2)\n",
    "\n",
    "                # set convergence threthold\n",
    "                thre = 1e-15\n",
    "                # set counter\n",
    "                ct = 0\n",
    "                while (gradL2 > thre) | (ct <100):\n",
    "                    \n",
    "                    grad = gradCal(Y, XAintercept, beta0)\n",
    "                    Hess = HessCal(Y, XAintercept, beta0)\n",
    "                    beta0 = beta0 + np.dot(np.linalg.inv(Hess), grad)\n",
    "                    grad_updated = gradCal(Y, XAintercept, beta0)\n",
    "                    ct = ct + 1\n",
    "                    gradL2 = np.mean(grad_updated**2)\n",
    "                return beta0\n",
    "\n",
    "    #========================fit oracle model========================\n",
    "    if Model == \"logcosh\":\n",
    "        betaOracle = fitLogCosh(Y, Xintercept)\n",
    "    else:\n",
    "        oracle_model = sm.GLM(endog = Y, exog = Xintercept, family = fitmodel)\n",
    "        oracle_results = oracle_model.fit()\n",
    "        betaOracle = oracle_results.params\n",
    "\n",
    "    # evaluation metrics for the oracle model\n",
    "    if reportAUC | reportll:\n",
    "        # linear predictor values from the initial value.\n",
    "        lp_oracle = np.dot(XEvalintercept, betaOracle )\n",
    "\n",
    "        # fitted probabilities\n",
    "        predCombined_oracle =invLinkVec(lp_oracle)\n",
    "\n",
    "    # evaluate the performance of the oracle model\n",
    "    if Model != \"logcosh\":\n",
    "        if reportAUC:\n",
    "            if reportll:\n",
    "                _, AUC_oracle, ll_oracle = eval(betaOracle, betaOracle, YEval, predCombined_oracle, loglikeli)\n",
    "            else:\n",
    "                _, AUC_oracle = eval(betaOracle, betaOracle, YEval, predCombined_oracle, loglikeli)\n",
    "        elif reportll:\n",
    "            _, ll_oracle = eval(betaOracle, betaOracle, YEval, predCombined_oracle, loglikeli)\n",
    "    #========================fit the initial model by A========================\n",
    "    # obtain initial values\n",
    "    # fit the model from A\n",
    "    if Model == \"logcosh\":\n",
    "        betaA = fitLogCosh(Y, XAintercept)\n",
    "    else:\n",
    "        modelA = sm.GLM(endog = Y, exog = XAintercept, family = fitmodel)\n",
    "        resultsA = modelA.fit()\n",
    "        betaA = resultsA.params\n",
    "\n",
    "    # initialize betaB by zeros\n",
    "    betaB = np.zeros((pB + 1))\n",
    "    #===============================Evaluation for the initial value=====================\n",
    "    # obtain the estimated coefficients from the initial values. take 0 for those from B\n",
    "    betaTemp = np.concatenate((betaA, np.repeat(0, pB)))\n",
    "\n",
    "    if reportAUC | reportll:\n",
    "        # linear predictor values from the initial value.\n",
    "        lp = np.dot(XEvalintercept, betaTemp)\n",
    "\n",
    "        # fitted probabilities\n",
    "        predCombined =invLinkVec(lp)\n",
    "\n",
    "    # evaluate the performance of the initial value\n",
    "    if reportAUC:\n",
    "        if reportll:\n",
    "            EuDis, AUC, ll = eval(betaOracle, betaTemp, YEval, predCombined, loglikeli)\n",
    "        else:\n",
    "            EuDis, AUC = eval(betaOracle, betaTemp, YEval, predCombined, loglikeli)\n",
    "    elif reportll:\n",
    "        EuDis, ll = eval(betaOracle, betaTemp, YEval, predCombined, loglikeli)\n",
    "    else:\n",
    "        EuDis = distance.euclidean(betaOracle, betaTemp)    \n",
    "\n",
    "\n",
    "    #store the evaluation results\n",
    "    # a list that stores the Euclidean distance between the assisted learning beta and oracl beta\n",
    "    EuDisList = [EuDis]\n",
    "\n",
    "    # a list that stores the AUC values\n",
    "    if reportAUC:\n",
    "        AUCList = [AUC]\n",
    "\n",
    "    # a list that stores the log-likelihoods\n",
    "    if reportll:\n",
    "        llList = [ll]\n",
    "\n",
    "    # an array that stores betaTemp\n",
    "    betaTempArray = betaTemp.reshape(1,-1)\n",
    "\n",
    "    #===============================start fitting the assisted learning model with kNum iteration=====================\n",
    "    for k in range(kNum):\n",
    "        # calculate the current step size \n",
    "        eta = etaFun(k, eta0)\n",
    "        # record most recent syncronized betas before the updates\n",
    "        betaB_old = betaB\n",
    "        betaA_old = betaA\n",
    "\n",
    "        if batchsize == 0:\n",
    "            XBintercept_minibatch = XBintercept\n",
    "            XAintercept_minibatch = XAintercept\n",
    "            Y_minibatch = Y\n",
    "        else:\n",
    "            # sample a minibatch\n",
    "            minibatch_ind = np.random.choice(X.shape[0], size=batchsize, replace=False)\n",
    "\n",
    "            XBintercept_minibatch = XBintercept[minibatch_ind, :]\n",
    "            XAintercept_minibatch = XAintercept[minibatch_ind, :]\n",
    "            Y_minibatch = Y[minibatch_ind]\n",
    "        # exchanged values\n",
    "        lpA = np.dot(XAintercept_minibatch, betaA)\n",
    "        lpB = np.dot(XBintercept_minibatch, betaB)\n",
    "        #===============================update of B===============================\n",
    "        for Qnum in range(Q):\n",
    "            # calculate the current lpB\n",
    "            lpB_Q = np.dot(XBintercept_minibatch, betaB)\n",
    "            if Model == \"logcosh\":\n",
    "                Grad_B = gradCalAssisted(Y_minibatch, XBintercept_minibatch, lpA, betaB)\n",
    "            else:\n",
    "\n",
    "                # calculate the fitted probabilities\n",
    "                lpCombined_Q_B = lpA + lpB_Q\n",
    "                predCombined_B =invLinkVec(lpCombined_Q_B)\n",
    "                # calculate the gradient, add the proximal term\n",
    "                Grad_B = np.dot((Y_minibatch - predCombined_B),XBintercept_minibatch)/len(Y_minibatch) - mu * (betaB - betaB_old)\n",
    "            betaB = betaB + eta * Grad_B\n",
    "        #===========================================================================\n",
    "        # if the pattern is sequential, A gets the udpated lpB from B. Otherwise, it gets the lpB before B's update\n",
    "        if TrainPattern == \"sequential\":\n",
    "            lpB = np.dot(XBintercept_minibatch, betaB)\n",
    "            betaB_old = betaB\n",
    "\n",
    "        #===============================update of A===============================\n",
    "        for Qnum in range(Q):\n",
    "            # calculate the current lpA\n",
    "            lpA_Q = np.dot(XAintercept_minibatch, betaA)\n",
    "            if Model == \"logcosh\":\n",
    "                Grad_A = gradCalAssisted(Y_minibatch, XAintercept_minibatch, lpB, betaA)\n",
    "            else:\n",
    "\n",
    "                # calculate the fitted probabilities\n",
    "                lpCombined_Q_A = lpA_Q + lpB\n",
    "                predCombined_A =invLinkVec(lpCombined_Q_A)\n",
    "                # calculate the gradient add the proximal term\n",
    "                Grad_A = np.dot((Y_minibatch - predCombined_A),XAintercept_minibatch)/len(Y_minibatch) - mu * (betaA - betaA_old)\n",
    "            betaA = betaA + eta * Grad_A\n",
    "\n",
    "        # combine the two coefficients from A and B\n",
    "        betaTemp = np.concatenate(([betaA[0] + betaB[0]],betaA[1:], betaB[1:]))\n",
    "        betaTempArray = np.concatenate((betaTempArray, betaTemp.reshape(1,-1)), axis = 0)\n",
    "        \n",
    "        if reportAUC | reportll:\n",
    "            # calculate the the linear predictor value\n",
    "            lp = np.dot(XEvalintercept, betaTemp)\n",
    "            # calculat the fitted probabilities from the asissted learning model   \n",
    "            predCombined =invLinkVec(lp)\n",
    "\n",
    "        # evaluate the performance of the current model\n",
    "            \n",
    "        if reportAUC:\n",
    "            if reportll:\n",
    "                EuDis, AUC, ll = eval(betaOracle, betaTemp, YEval, predCombined, loglikeli)\n",
    "            else:\n",
    "                EuDis, AUC = eval(betaOracle, betaTemp, YEval, predCombined, loglikeli)\n",
    "        elif reportll:\n",
    "            EuDis, ll = eval(betaOracle, betaTemp, YEval, predCombined, loglikeli)\n",
    "        else:\n",
    "            EuDis = distance.euclidean(betaOracle, betaTemp)    \n",
    "    \n",
    "\n",
    "        if reportAUC:\n",
    "            AUCList.append(AUC)\n",
    "        if reportll:\n",
    "            llList.append(ll)\n",
    "        EuDisList.append(EuDis)\n",
    "    # return the evaluation results\n",
    "    if reportAUC:\n",
    "        if reportll:\n",
    "            return AUCList, EuDisList, llList, AUC_oracle, ll_oracle\n",
    "        else:\n",
    "            return AUCList, EuDisList, AUC_oracle\n",
    "    elif reportll:\n",
    "        return EuDisList, llList, ll_oracle\n",
    "    else:\n",
    "        return EuDisList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit VFL\n",
    "kNum = 50\n",
    "reportAUC = True\n",
    "reportll = False\n",
    "\n",
    "Model = \"logistic\"\n",
    "TrainPattern = \"parallel\"\n",
    "# batchsize = 0\n",
    "batchsizeList = [0, 32]\n",
    "\n",
    "# number of eta0 values\n",
    "numEta0s = 20 \n",
    "minval = 10**(-5)\n",
    "maxval = 0.1\n",
    "#eta0List = math.e**(np.linspace(math.log(minval), math.log(maxval), num=numEta0s))[::-1]\n",
    "eta0List = math.e**(np.linspace(math.log(minval), math.log(maxval), num=numEta0s))\n",
    "# function to generate the decay learning rate, where k is the iteration number\n",
    "def etaFun(k, eta0):\n",
    "    eta = eta0/math.sqrt(k + 1)\n",
    "    return(eta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n"
     ]
    }
   ],
   "source": [
    "#=========================================training of FedBCD==============================\n",
    "\n",
    "muList = [0, 0.1]\n",
    "#muList = [0.1]\n",
    "for batchsize in batchsizeList:\n",
    "    for mu in muList:\n",
    "        if mu == 0:\n",
    "            #QList = [1, 5, 10, 25]\n",
    "            # do not train fedSGD together with fedBCD\n",
    "            QList = [5, 10, 25]\n",
    "        else:\n",
    "            QList = [5, 10, 25]\n",
    "        for Q in QList:\n",
    "            for eta0 in eta0List:\n",
    "                try: \n",
    "                    AUCList, EuDisList, AUC_oracle = fitVFL(Model, batchsize, Q, mu, eta0, etaFun, TrainPattern, Y, XA, XB, YTest, XTest, kNum, fitmodel, loglikeli, reportAUC, reportll)\n",
    "                    #==============================================export the results===============================================\n",
    "                    result = {\"EuDis\": EuDisList, \"AUC\": AUCList,  \"AUC_oracle\": AUC_oracle}\n",
    "\n",
    "                    pickle.dump(result, open(\"VFL_result_batchsize\" + str(batchsize) + \"_Q_\" + str(Q) + \"_mu_\" + str(mu) + \"_eta0_\" + str(eta0) + \"TrainPattern\" + TrainPattern + \"_dic.p\", \"wb\"))\n",
    "\n",
    "                except BaseException:\n",
    "                    print('The step size is too large, stop and try other settings')\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The step size is too large, stop and try other settings\n",
      "The step size is too large, stop and try other settings\n"
     ]
    }
   ],
   "source": [
    "#=========================================training of FedSGD==============================\n",
    "# apply a set of larger step sizes\n",
    "minval2 = 10**(-5)\n",
    "maxval2 = 0.1\n",
    "#eta0List = math.e**(np.linspace(math.log(minval), math.log(maxval), num=numEta0s))[::-1]\n",
    "eta0List2 = math.e**(np.linspace(math.log(minval2), math.log(maxval2), num=numEta0s))\n",
    "\n",
    "#batchsize = 0\n",
    "Q = 1\n",
    "mu = 0\n",
    "for batchsize in batchsizeList:\n",
    "    for eta0 in eta0List2:\n",
    "        try: \n",
    "            AUCList, EuDisList, AUC_oracle = fitVFL(Model, batchsize, Q, mu, eta0, etaFun, TrainPattern, Y, XA, XB, YTest, XTest, kNum, fitmodel, loglikeli, reportAUC, reportll)\n",
    "            #==============================================export the results===============================================\n",
    "            result = {\"EuDis\": EuDisList, \"AUC\": AUCList,  \"AUC_oracle\": AUC_oracle}\n",
    "\n",
    "            pickle.dump(result, open(\"VFL_result_batchsize\" + str(batchsize) + \"_Q_\" + str(Q) + \"_mu_\" + str(mu) + \"_eta0_\" + str(eta0) + \"TrainPattern\" + TrainPattern + \"_dic.p\", \"wb\"))\n",
    "\n",
    "        except BaseException:\n",
    "            print('The step size is too large, stop and try other settings')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultVals = {\"maxval\": maxval, \"minval\": minval, \"maxval2\": maxval2, \"minval2\": minval2}\n",
    "\n",
    "pickle.dump(resultVals, open(\"minmaxVals_dic.p\", \"wb\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEU1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe88e7b441143892554f327074c8bcade18da71d893ccd3a35749aabccc17428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
