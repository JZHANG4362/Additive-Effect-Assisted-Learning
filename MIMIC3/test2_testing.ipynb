{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "#=============================read data=====================================\n",
    "\n",
    "fname = \"data_preprocessed_dic.p\"\n",
    "infile = open(fname, 'rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "\n",
    "\n",
    "Y = new_dict['Y_tmp']\n",
    "\n",
    "\n",
    "\n",
    "XA = new_dict['XA_tmp']\n",
    "XB = new_dict['XB_tmp']\n",
    "# # drop linearly dependent columns\n",
    "# q1,r1 = np.linalg.qr(XA_tmp.T)\n",
    "# XA = XA_tmp[:, np.abs(np.diag(r1))>=1e-10]\n",
    "\n",
    "# q2,r2 = np.linalg.qr(XB_tmp.T)\n",
    "# XB = XB_tmp[:, np.abs(np.diag(r2))>=1e-10]\n",
    "\n",
    "# add intercept terms that consist of 1 to the predictor datasets\n",
    "# XA matrix with intercept\n",
    "XAintercept = sm.add_constant(XA)\n",
    "\n",
    "Model = 'logistic'\n",
    "# output AUC\n",
    "reportAUC = True\n",
    "# do not report ll \n",
    "reportll = True\n",
    "#========================Define the log-likelihood for evaluation========================\n",
    "# logistic regression log-likelihood\n",
    "def loglikeli(YEval, predCombined):\n",
    "    # # clip the predicted probability to avoid it getting too close to 0 or 1\n",
    "    # predCombined_clipped = np.clip(predCombined, 1e-15, (1-1e-15))\n",
    "\n",
    "    predCombined_clipped = predCombined\n",
    "    ll = np.mean(YEval * np.log(predCombined_clipped ) + (1-YEval) * np.log(1 - predCombined_clipped ))\n",
    "    return ll\n",
    "#=============================function to calculate the inverse logit link============================\n",
    "def invLink(lp):\n",
    "    pv = 1/(1 + math.exp(-lp))\n",
    "    return pv\n",
    "\n",
    "# vectorize the function\n",
    "invLinkVec = np.vectorize(invLink)\n",
    "\n",
    "#======================function to generate response data given linear predictors=====================\n",
    "def ResponseGen(lp):\n",
    "    pv = invLink(lp)\n",
    "    binoRand = np.random.binomial(1, pv, 1)\n",
    "    return binoRand\n",
    "\n",
    "# vectorize the function\n",
    "ResponseGenVec = np.vectorize(ResponseGen)\n",
    "\n",
    "#=============================specify the model to fit============================\n",
    "fitmodel = sm.families.Binomial(link = sm.families.links.logit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TestFun(testtype, Lapscale, K, Y, XAintercept, XB):\n",
    "    # sample size\n",
    "    n = XAintercept.shape[0]\n",
    "    if Model == \"logcosh\":\n",
    "        #========================define functions for fitting log-cosh====================\n",
    "        logcoshA = 0.3\n",
    "        def gradCal(Y, XAintercept, beta0):\n",
    "            # get the residual\n",
    "            resid = Y - np.dot(XAintercept, beta0)\n",
    "            # calculate the gradient\n",
    "            grad = np.mean((np.tanh(logcoshA * resid).reshape(n,-1)) * XAintercept, axis = 0)\n",
    "            return(grad)\n",
    "\n",
    "        def HessCal(Y, XAintercept, beta0):\n",
    "            # get the residual\n",
    "            resid = Y - np.dot(XAintercept, beta0)\n",
    "            # calculate the Hessian\n",
    "            Hess = logcoshA * n**(-1) * np.dot(np.dot(np.transpose(XAintercept), np.diag(np.cosh(logcoshA * resid)**(-1))), XAintercept)\n",
    "            return(Hess)\n",
    "        def fitLogCosh(Y, XAintercept):\n",
    "            # fit a linear regression as the initial value\n",
    "            model = sm.GLM(endog = Y, exog = XAintercept, family = fitmodel)\n",
    "            # get initial value\n",
    "            beta0 = model.fit().params\n",
    "\n",
    "            # evaluate the gradient\n",
    "            grad_updated = gradCal(Y, XAintercept, beta0)\n",
    "            gradL2 = np.mean(grad_updated**2)\n",
    "\n",
    "            # set convergence threthold\n",
    "            thre = 1e-15\n",
    "            # set counter\n",
    "            ct = 0\n",
    "            while (gradL2 > thre) | (ct <100):\n",
    "                grad = gradCal(Y, XAintercept, beta0)\n",
    "                Hess = HessCal(Y, XAintercept, beta0)\n",
    "                beta0 = beta0 + np.dot(np.linalg.inv(Hess), grad)\n",
    "                grad_updated = gradCal(Y, XAintercept, beta0)\n",
    "                ct = ct + 1\n",
    "                gradL2 = np.mean(grad_updated**2)\n",
    "            return beta0\n",
    "    if testtype == \"likelihoodRatio\":\n",
    "        #========================fit the H0 model========================\n",
    "        # A fits the initial model\n",
    "        modelH0 = sm.GLM(endog = Y, exog = XAintercept, family = fitmodel)\n",
    "        modelH0_results = modelH0.fit()\n",
    "\n",
    "    # list of p-values\n",
    "    pVList = []\n",
    "    # list of rejection results\n",
    "    rejList = []\n",
    "\n",
    "    # initialize the predictor data for A\n",
    "    XAintercept_add = XAintercept\n",
    "    for i in range(K):\n",
    "        # obtain the random vector u\n",
    "        u = np.random.normal(loc = 0, scale = 1, size = (XB.shape[1], 1))\n",
    "        uXB_tmp = np.dot(XB, u)\n",
    "        # add noise\n",
    "        uXB = uXB_tmp + np.random.laplace(loc=0, scale=Lapscale, size=uXB_tmp.shape)\n",
    "  \n",
    "        #XAintercept_add_tmp = np.concatenate((XAintercept_add, uXB.reshape(uXB.shape[0],-1)), axis = 1)\n",
    "        XAintercept_add = np.concatenate((XAintercept_add, uXB.reshape(uXB.shape[0],-1)), axis = 1)\n",
    "        # #=============delete linearly dependent columns=============\n",
    "        # q,r = np.linalg.qr(XAintercept_add_tmp.T)\n",
    "\n",
    "        # XAintercept_add = XAintercept_add_tmp[:, np.abs(np.diag(r))>=1e-10]\n",
    "        #========================fit the H1 model========================\n",
    "        # A fits the H1 model after receiving the data\n",
    "        if Model == \"logcosh\":\n",
    "            betaFitted = fitLogCosh(Y, XAintercept_add)\n",
    "        else:\n",
    "            modelH1 = sm.GLM(endog = Y, exog = XAintercept_add, family = fitmodel)\n",
    "            modelH1_results = modelH1.fit()\n",
    "            betaFitted = modelH1_results.params\n",
    "        # test statistic\n",
    "        if testtype == \"likelihoodRatio\":\n",
    "            stat = 2 * (modelH1_results.llf - modelH0_results.llf)\n",
    "        elif testtype == \"Wald\":\n",
    "            # linear predictor\n",
    "            lp = np.dot(XAintercept_add, betaFitted)\n",
    "            \n",
    "            # prediction result\n",
    "            if Model == \"logcosh\":\n",
    "                prd =  np.dot(XAintercept_add, betaFitted)\n",
    "            else:\n",
    "                prd = modelH1_results.predict(XAintercept_add)\n",
    "            # number of predictors from A\n",
    "            pA = XAintercept.shape[1]\n",
    "            # fitted coefficients\n",
    "            # prediction result\n",
    "            betaU =betaFitted[pA: ]\n",
    "\n",
    "            \n",
    "            stat = statCal(Y, XAintercept_add, pA, prd, betaU, lp)\n",
    "        df = (XAintercept_add.shape[1] - XAintercept.shape[1])\n",
    "# #########\n",
    "#         print(df)\n",
    "#         df = i + 1\n",
    "#         # print(modelH1_results.llf)\n",
    "#         # print(modelH0_results.llf)\n",
    "#         print(i+1)\n",
    "# #########\n",
    "        # p value\n",
    "        pV = 1 - scipy.stats.chi2.cdf(stat, df)\n",
    "        pVList.append(pV)\n",
    "        # rejection\n",
    "        rej = pV < 0.05\n",
    "        rejList.append(rej)\n",
    "        \n",
    "    print(rejList)\n",
    "    print(pVList)\n",
    "    # [True, True, True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True]\n",
      "[9.159139002790084e-10, 2.1220133428201393e-09, 4.6413983767479294e-12]\n",
      "[True, True, True]\n",
      "[0.0015097081663844047, 0.0003195817679734203, 1.1102230246251565e-16]\n",
      "[True, True, True]\n",
      "[0.0005485868072927502, 0.00630474822422511, 0.01961008254493568]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(20)\n",
    "for Lapscale in [0, 0.1, 0.5]:\n",
    "    TestFun('Wald', Lapscale, 3, Y, XAintercept, XB)\n",
    "\n",
    "# [True, True, True]\n",
    "# [9.159139002790084e-10, 2.1220133428201393e-09, 4.6413983767479294e-12]\n",
    "# [True, True, True]\n",
    "# [0.0015097081663844047, 0.0003195817679734203, 1.1102230246251565e-16]\n",
    "# [True, True, True]\n",
    "# [0.0005485868072927502, 0.00630474822422511, 0.01961008254493568]\n",
    "# >>> 0.05/3\n",
    "# 0.016666666666666666"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEU1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe88e7b441143892554f327074c8bcade18da71d893ccd3a35749aabccc17428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
