require(glmnet)
lambMin <- cv.glmnet(Xmat, dat$y, nfolds = nK, family = "gaussian", alpha = 0)$lambda.min
mod <- glmnet(Xmat, dat$y, family = "gaussian", lambda = lambMin, alpha = 0)
coeff <- c(mod$a0, as.numeric(mod$beta))
oracle_MSE <- mean((dat_Pred$y - (mod$a0 + Xmat_Pred %*% mod$beta))^2)
MSEmat[j,] <- mod_assisted$MSE
MSEmat_oracle[j] <- oracle_MSE
lambdaAssVec[j] <- lambdaAssisted
lambdaOraVec[j] <- lambMin
}
result <- list(MSE=MSEmat, MSE_oracle=MSEmat_oracle,
lambdaAssVec=lambdaAssVec, lambdaOraVec=lambdaOraVec)
return(result)
}
# Covariance
sigList <- c(0, 0.1, 0.25)
# size of the training data
nList <- c(20, 200)
# size of the prediction data
nPred <- 10^6
# number of training rounds
nit <- 5
# number of CV folds in selecting lambda
nK <- 4
# number of replications
nrep <- 100
#nrep <- 100
# distribution of predictors
Xdist = "uniform"
# patterns of data-generating coefficients
betaVec <- rbind(c(12,12,0), c(8,8,8), c(4,16,4))
set.seed(20)
for (nInd in seq_along(nList)){
n <- nList[nInd]
for (sigInd in seq_along(sigList)){
sig <- sigList[sigInd]
for (betasetting in c(1:3)){
# generate the parameters for betas
be1 <- rep(1,betaVec[betasetting,1])
be2 <- rep(0,betaVec[betasetting,2])
be3 <- rep(1,betaVec[betasetting,3])
TrainingRes <- TestFun(nrep=nrep,
n=n,
Xdist=Xdist,
nPred=nPred,
sig=sig,
nit=nit,
nK=nK,
be1=be1,
be2=be2,
be3=be3)
filename <- paste("_betasetting_", betasetting, "n_", n, "_", "sig_", sig, "_Xdist_",  Xdist,  sep="")
write.csv(TrainingRes$MSE, paste("data/ridgeTrainingAssistedMSE_", filename, ".csv", sep=""))
write.csv(TrainingRes$MSE_oracle, paste("data/ridgeTrainingOracleMSE_", filename, ".csv", sep=""))
write.csv(TrainingRes$lambdaAssVec, paste("data/ridgeTrainingAssistedLam_", filename, ".csv", sep=""))
write.csv(TrainingRes$lambdaOraVec, paste("data/ridgeTrainingOracleLam_", filename, ".csv", sep=""))
}
}
}
#====================================================
#Note: since the smallest n=20, nfolds is set to be 4
#====================================================
setwd("/Users/mikezhang/Documents/学习19fall/RA/GLM_privacy/codes/AssistedLearning_codes/simulations/Step2_Training/ridgeFitting")
library(MASS)
library(caret)
library(glmnet)
library(expm)
datGen <- function(n, Xdist, sigMat, be1, be2, be3){
# number of predictors from user 1
p1 <- length(be1)
# number of predictors from user 2
p2 <- length(be2)
# number of shared predictors
p3 <- length(be3)
# total number of predictors
p <- p1 + p2 + p3
# generate predictors data
if (Xdist == "normal"){
Xdat_tmp <- matrix(rnorm((n*p)), nrow = n, ncol = p)
}else if (Xdist == "uniform"){
Xdat_tmp <- matrix(runif((n*p)), nrow = n, ncol = p)
}
Xdat <- Xdat_tmp %*% sqrtm(sigMat)
x1 <- Xdat[,c(1 : p1)]
x2 <- Xdat[,c((p1+1) : (p1 + p2))]
if (p3 != 0){
x3 <- Xdat[,c((p1 + p2 + 1) : p)]
}
# generate response data (coefficients are all 1)
if (p3 != 0){
eta <-  as.numeric(x1 %*% be1 + x2%*% be2 + x3%*%be3)
}else{
eta <-  as.numeric(x1 %*% be1 + x2%*% be2)
}
y <- rnorm(n, mean = eta, sd = 1)
if (p3 != 0){
# full data set
dat <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3)
# data for user 1
dat1 <- data.frame(y = y, x1 = x1, x3 = x3)
# data for user 2
dat2 <- data.frame(y = y, x2 = x2, x3 = x3)
}else{
# full data set
dat <- data.frame(y = y, x1 = x1, x2 = x2)
# data for user 1
dat1 <- data.frame(y = y, x1 = x1)
# data for user 2
dat2 <- data.frame(y = y, x2 = x2)
}
return(list(dat = dat, dat1 = dat1, dat2 = dat2))
}
AssistedTraining <- function(lamTest, nit, Xmat1, Xmat2, y, Xmat1_pred, Xmat2_pred, y_pred, p1,p2,p3){
# matrix to store beta values
betaMat <- matrix(nrow=(nit + 1), ncol = (1 + p1 + p2 + p3))
MSEvec <- numeric((nit + 1))
# initialize
mod1 <- glmnet(Xmat1, y, family = "gaussian", lambda =  lamTest , alpha = 0)
print(length(c((mod1$a0 + 0), mod1$beta[1:p1], rep(0,p2), (mod1$beta[-c(1:p1)] ))))
print(dim(betaMat))
print(c(p1,p2,p3))
betaMat[1,] <- c((mod1$a0 + 0), mod1$beta[1:p1], rep(0,p2), (mod1$beta[-c(1:p1)] ))
predV <- mod1$a0 +  Xmat1_pred %*%  mod1$beta
MSEvec[1] <- mean((y_pred - predV)^2)
for (i in c(1:nit)){
# update eta
eta1 <- as.numeric(mod1$a0 +  Xmat1 %*%  mod1$beta)
# update beta2
mod2 <- glmnet(Xmat2, y, offset = eta1, family = "gaussian", lambda =  lamTest , alpha = 0)
# update eta
eta2 <- as.numeric(mod2$a0 +  Xmat2 %*%  mod2$beta)
# update beta1
mod1 <- glmnet(Xmat1, y, offset = eta2, family = "gaussian", lambda =  lamTest , alpha = 0)
# combined coefficients
betaMat[i,] <- c((mod1$a0 + mod2$a0), mod1$beta[1:p1], mod2$beta[1:p2], (mod1$beta[(p1 + 1) : (p1 + p3)] + mod2$beta[(p2 + 1) : (p2 + p3)]))
# predicted values
predV <- as.numeric((mod1$a0 + mod2$a0) +  Xmat1_pred %*%  mod1$beta + Xmat2_pred %*%  mod2$beta)
# mean squared error
MSEvec[(i + 1)] <- mean((y_pred - predV)^2)
}
result <- list(beta=betaMat, MSE=MSEvec)
}
TestFun <- function(nrep, n, Xdist, nPred, sig, nit, nK, be1, be2, be3){
MSEmat <- matrix(nrow = nrep, ncol = (nit + 1))
MSEmat_oracle <- numeric(nrep)
lambdaAssVec <- numeric(nrep)
lambdaOraVec <- numeric(nrep)
# take the "be" values as the means instead of the coefficient values
be1_mean <- be1
be2_mean <- be2
be3_mean <- be3
# lengths of betas
p1 <- length(be1_mean)
p2 <- length(be2_mean)
p3 <- length(be3_mean)
p <- p1 + p2 + p3
for (j in c(1:nrep)){
message(j)
# randomly generate coefficients
be1 <- mvrnorm(mu = be1_mean, Sigma = diag(rep(1, length(be1_mean))))
be2 <- mvrnorm(mu = be2_mean, Sigma = diag(rep(1, length(be2_mean))))
if (p3 != 0){
be3 <- mvrnorm(mu = be3_mean, Sigma = diag(rep(1, length(be3_mean))))
}
# covariance matrix of the predictors
ar1_cor <- function(n, rho) {
exponent <- abs(matrix(1:n - 1, nrow = n, ncol = n, byrow = TRUE) -
(1:n - 1))
rho^exponent
}
sigMat <- ar1_cor(p, sig)
#=============== Generate data set for training ==============
datList <- datGen(n = n, Xdist = Xdist, sigMat = sigMat, be1 = be1, be2 = be2, be3 = be3)
dat <- datList$dat
# predictor observations
Xmat <- as.matrix(dat[,-1])
# data for user 1
dat1 <- datList$dat1
# predictor observations
Xmat1 <- as.matrix(dat1[,-1])
# data for user 2
dat2 <- datList$dat2
# predictor observations
Xmat2 <- as.matrix(dat2[,-1])
#=============== Generate new data set for prediction ==============
datList_Pred <- datGen(n = nPred, Xdist = Xdist, sigMat = sigMat, be1 = be1, be2 = be2, be3 = be3)
dat_Pred <- datList_Pred$dat
# predictor observations
Xmat_Pred <- as.matrix(dat_Pred[,-1])
# data for user 1
dat1_Pred <- datList_Pred$dat1
# predictor observations
Xmat1_Pred <- as.matrix(dat1_Pred[,-1])
# data for user 2
dat2_Pred <- datList_Pred$dat2
# predictor observations
Xmat2_Pred <- as.matrix(dat2_Pred[,-1])
# Obtain a list of lambdas from Xmat1
CVA <- cv.glmnet(Xmat1, dat$y, nfolds = nK, family = "gaussian", alpha = 0)
lambdaList <- CVA$lambda
# split the dataset
CVInd <- createMultiFolds(c(1:n), k = nK, times = 1)
# Vector to store MSE for each lambda value
MSEvec <- numeric(length(lambdaList))
for (lamIndex in seq_along(lambdaList)){
lamTest <- lambdaList[lamIndex]
# collect MSE
MSE_tmp <- numeric(nK)
for (k in c(1:nK)){
# fit the model
test <- AssistedTraining(lamTest=lamTest, nit=nit, Xmat1=Xmat1[CVInd[[k]],], Xmat2= Xmat2[CVInd[[k]],], y=dat$y[CVInd[[k]]], Xmat1_pred=Xmat1[-CVInd[[k]],], Xmat2_pred=Xmat2[-CVInd[[k]],], y_pred=dat$y[-CVInd[[k]]], p1=p1,p2=p2,p3=p3)
# calculate the MSE
MSE_tmp[k] <- test$MSE[length(test$MSE)]
}
MSEvec[lamIndex] <- mean(MSE_tmp)
}
# select the lambda with the smallest MSE from CV
minInd <- which(MSEvec == min(MSEvec))
lambdaAssisted <- lambdaList[minInd]
#================ train the assisted learning model=================
mod_assisted <- AssistedTraining(lamTest=lambdaAssisted, nit=nit, Xmat1=Xmat1, Xmat2= Xmat2, y=dat$y, Xmat1_pred=Xmat1_Pred, Xmat2_pred=Xmat2_Pred, y_pred=dat_Pred$y, p1=p1, p2=p2, p3=p3)
#================ calculate beta from the oracle model=================
require(glmnet)
lambMin <- cv.glmnet(Xmat, dat$y, nfolds = nK, family = "gaussian", alpha = 0)$lambda.min
mod <- glmnet(Xmat, dat$y, family = "gaussian", lambda = lambMin, alpha = 0)
coeff <- c(mod$a0, as.numeric(mod$beta))
oracle_MSE <- mean((dat_Pred$y - (mod$a0 + Xmat_Pred %*% mod$beta))^2)
MSEmat[j,] <- mod_assisted$MSE
MSEmat_oracle[j] <- oracle_MSE
lambdaAssVec[j] <- lambdaAssisted
lambdaOraVec[j] <- lambMin
}
result <- list(MSE=MSEmat, MSE_oracle=MSEmat_oracle,
lambdaAssVec=lambdaAssVec, lambdaOraVec=lambdaOraVec)
return(result)
}
# Covariance
sigList <- c(0, 0.1, 0.25)
# size of the training data
nList <- c(20, 200)
# size of the prediction data
nPred <- 10^6
# number of training rounds
nit <- 5
# number of CV folds in selecting lambda
nK <- 4
# number of replications
nrep <- 100
#nrep <- 100
# distribution of predictors
Xdist = "uniform"
# patterns of data-generating coefficients
betaVec <- rbind(c(12,12,0), c(8,8,8), c(4,16,4))
set.seed(20)
for (nInd in seq_along(nList)){
n <- nList[nInd]
for (sigInd in seq_along(sigList)){
sig <- sigList[sigInd]
for (betasetting in c(1:3)){
# generate the parameters for betas
be1 <- rep(1,betaVec[betasetting,1])
be2 <- rep(0,betaVec[betasetting,2])
be3 <- rep(1,betaVec[betasetting,3])
TrainingRes <- TestFun(nrep=nrep,
n=n,
Xdist=Xdist,
nPred=nPred,
sig=sig,
nit=nit,
nK=nK,
be1=be1,
be2=be2,
be3=be3)
filename <- paste("_betasetting_", betasetting, "n_", n, "_", "sig_", sig, "_Xdist_",  Xdist,  sep="")
write.csv(TrainingRes$MSE, paste("data/ridgeTrainingAssistedMSE_", filename, ".csv", sep=""))
write.csv(TrainingRes$MSE_oracle, paste("data/ridgeTrainingOracleMSE_", filename, ".csv", sep=""))
write.csv(TrainingRes$lambdaAssVec, paste("data/ridgeTrainingAssistedLam_", filename, ".csv", sep=""))
write.csv(TrainingRes$lambdaOraVec, paste("data/ridgeTrainingOracleLam_", filename, ".csv", sep=""))
}
}
}
#====================================================
#Note: since the smallest n=20, nfolds is set to be 4
#====================================================
setwd("/Users/mikezhang/Documents/学习19fall/RA/GLM_privacy/codes/AssistedLearning_codes/simulations/Step2_Training/ridgeFitting")
library(MASS)
library(caret)
library(glmnet)
library(expm)
datGen <- function(n, Xdist, sigMat, be1, be2, be3){
# number of predictors from user 1
p1 <- length(be1)
# number of predictors from user 2
p2 <- length(be2)
# number of shared predictors
p3 <- length(be3)
# total number of predictors
p <- p1 + p2 + p3
# generate predictors data
if (Xdist == "normal"){
Xdat_tmp <- matrix(rnorm((n*p)), nrow = n, ncol = p)
}else if (Xdist == "uniform"){
Xdat_tmp <- matrix(runif((n*p)), nrow = n, ncol = p)
}
Xdat <- Xdat_tmp %*% sqrtm(sigMat)
x1 <- Xdat[,c(1 : p1)]
x2 <- Xdat[,c((p1+1) : (p1 + p2))]
if (p3 != 0){
x3 <- Xdat[,c((p1 + p2 + 1) : p)]
}
# generate response data (coefficients are all 1)
if (p3 != 0){
eta <-  as.numeric(x1 %*% be1 + x2%*% be2 + x3%*%be3)
}else{
eta <-  as.numeric(x1 %*% be1 + x2%*% be2)
}
y <- rnorm(n, mean = eta, sd = 1)
if (p3 != 0){
# full data set
dat <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3)
# data for user 1
dat1 <- data.frame(y = y, x1 = x1, x3 = x3)
# data for user 2
dat2 <- data.frame(y = y, x2 = x2, x3 = x3)
}else{
# full data set
dat <- data.frame(y = y, x1 = x1, x2 = x2)
# data for user 1
dat1 <- data.frame(y = y, x1 = x1)
# data for user 2
dat2 <- data.frame(y = y, x2 = x2)
}
return(list(dat = dat, dat1 = dat1, dat2 = dat2))
}
AssistedTraining <- function(lamTest, nit, Xmat1, Xmat2, y, Xmat1_pred, Xmat2_pred, y_pred, p1,p2,p3){
# matrix to store beta values
betaMat <- matrix(nrow=(nit + 1), ncol = (1 + p1 + p2 + p3))
MSEvec <- numeric((nit + 1))
# initialize
mod1 <- glmnet(Xmat1, y, family = "gaussian", lambda =  lamTest , alpha = 0)
print(length(c((mod1$a0 + 0), mod1$beta[1:p1], rep(0,p2), (mod1$beta[-c(1:p1)] ))))
print(dim(betaMat))
print(c(p1,p2,p3))
betaMat[1,] <- c((mod1$a0 + 0), mod1$beta[1:p1], rep(0,p2), (mod1$beta[-c(1:p1)] ))
predV <- mod1$a0 +  Xmat1_pred %*%  mod1$beta
MSEvec[1] <- mean((y_pred - predV)^2)
for (i in c(1:nit)){
# update eta
eta1 <- as.numeric(mod1$a0 +  Xmat1 %*%  mod1$beta)
# update beta2
mod2 <- glmnet(Xmat2, y, offset = eta1, family = "gaussian", lambda =  lamTest , alpha = 0)
# update eta
eta2 <- as.numeric(mod2$a0 +  Xmat2 %*%  mod2$beta)
# update beta1
mod1 <- glmnet(Xmat1, y, offset = eta2, family = "gaussian", lambda =  lamTest , alpha = 0)
# combined coefficients
betaMat[i,] <- c((mod1$a0 + mod2$a0), mod1$beta[1:p1], mod2$beta[1:p2], (mod1$beta[-c(1:p1)] + mod2$beta[-c(1:p2)]))
# predicted values
predV <- as.numeric((mod1$a0 + mod2$a0) +  Xmat1_pred %*%  mod1$beta + Xmat2_pred %*%  mod2$beta)
# mean squared error
MSEvec[(i + 1)] <- mean((y_pred - predV)^2)
}
result <- list(beta=betaMat, MSE=MSEvec)
}
TestFun <- function(nrep, n, Xdist, nPred, sig, nit, nK, be1, be2, be3){
MSEmat <- matrix(nrow = nrep, ncol = (nit + 1))
MSEmat_oracle <- numeric(nrep)
lambdaAssVec <- numeric(nrep)
lambdaOraVec <- numeric(nrep)
# take the "be" values as the means instead of the coefficient values
be1_mean <- be1
be2_mean <- be2
be3_mean <- be3
# lengths of betas
p1 <- length(be1_mean)
p2 <- length(be2_mean)
p3 <- length(be3_mean)
p <- p1 + p2 + p3
for (j in c(1:nrep)){
message(j)
# randomly generate coefficients
be1 <- mvrnorm(mu = be1_mean, Sigma = diag(rep(1, length(be1_mean))))
be2 <- mvrnorm(mu = be2_mean, Sigma = diag(rep(1, length(be2_mean))))
if (p3 != 0){
be3 <- mvrnorm(mu = be3_mean, Sigma = diag(rep(1, length(be3_mean))))
}
# covariance matrix of the predictors
ar1_cor <- function(n, rho) {
exponent <- abs(matrix(1:n - 1, nrow = n, ncol = n, byrow = TRUE) -
(1:n - 1))
rho^exponent
}
sigMat <- ar1_cor(p, sig)
#=============== Generate data set for training ==============
datList <- datGen(n = n, Xdist = Xdist, sigMat = sigMat, be1 = be1, be2 = be2, be3 = be3)
dat <- datList$dat
# predictor observations
Xmat <- as.matrix(dat[,-1])
# data for user 1
dat1 <- datList$dat1
# predictor observations
Xmat1 <- as.matrix(dat1[,-1])
# data for user 2
dat2 <- datList$dat2
# predictor observations
Xmat2 <- as.matrix(dat2[,-1])
#=============== Generate new data set for prediction ==============
datList_Pred <- datGen(n = nPred, Xdist = Xdist, sigMat = sigMat, be1 = be1, be2 = be2, be3 = be3)
dat_Pred <- datList_Pred$dat
# predictor observations
Xmat_Pred <- as.matrix(dat_Pred[,-1])
# data for user 1
dat1_Pred <- datList_Pred$dat1
# predictor observations
Xmat1_Pred <- as.matrix(dat1_Pred[,-1])
# data for user 2
dat2_Pred <- datList_Pred$dat2
# predictor observations
Xmat2_Pred <- as.matrix(dat2_Pred[,-1])
# Obtain a list of lambdas from Xmat1
CVA <- cv.glmnet(Xmat1, dat$y, nfolds = nK, family = "gaussian", alpha = 0)
lambdaList <- CVA$lambda
# split the dataset
CVInd <- createMultiFolds(c(1:n), k = nK, times = 1)
# Vector to store MSE for each lambda value
MSEvec <- numeric(length(lambdaList))
for (lamIndex in seq_along(lambdaList)){
lamTest <- lambdaList[lamIndex]
# collect MSE
MSE_tmp <- numeric(nK)
for (k in c(1:nK)){
# fit the model
test <- AssistedTraining(lamTest=lamTest, nit=nit, Xmat1=Xmat1[CVInd[[k]],], Xmat2= Xmat2[CVInd[[k]],], y=dat$y[CVInd[[k]]], Xmat1_pred=Xmat1[-CVInd[[k]],], Xmat2_pred=Xmat2[-CVInd[[k]],], y_pred=dat$y[-CVInd[[k]]], p1=p1,p2=p2,p3=p3)
# calculate the MSE
MSE_tmp[k] <- test$MSE[length(test$MSE)]
}
MSEvec[lamIndex] <- mean(MSE_tmp)
}
# select the lambda with the smallest MSE from CV
minInd <- which(MSEvec == min(MSEvec))
lambdaAssisted <- lambdaList[minInd]
#================ train the assisted learning model=================
mod_assisted <- AssistedTraining(lamTest=lambdaAssisted, nit=nit, Xmat1=Xmat1, Xmat2= Xmat2, y=dat$y, Xmat1_pred=Xmat1_Pred, Xmat2_pred=Xmat2_Pred, y_pred=dat_Pred$y, p1=p1, p2=p2, p3=p3)
#================ calculate beta from the oracle model=================
require(glmnet)
lambMin <- cv.glmnet(Xmat, dat$y, nfolds = nK, family = "gaussian", alpha = 0)$lambda.min
mod <- glmnet(Xmat, dat$y, family = "gaussian", lambda = lambMin, alpha = 0)
coeff <- c(mod$a0, as.numeric(mod$beta))
oracle_MSE <- mean((dat_Pred$y - (mod$a0 + Xmat_Pred %*% mod$beta))^2)
MSEmat[j,] <- mod_assisted$MSE
MSEmat_oracle[j] <- oracle_MSE
lambdaAssVec[j] <- lambdaAssisted
lambdaOraVec[j] <- lambMin
}
result <- list(MSE=MSEmat, MSE_oracle=MSEmat_oracle,
lambdaAssVec=lambdaAssVec, lambdaOraVec=lambdaOraVec)
return(result)
}
# Covariance
sigList <- c(0, 0.1, 0.25)
# size of the training data
nList <- c(20, 200)
# size of the prediction data
nPred <- 10^6
# number of training rounds
nit <- 5
# number of CV folds in selecting lambda
nK <- 4
# number of replications
nrep <- 100
#nrep <- 100
# distribution of predictors
Xdist = "uniform"
# patterns of data-generating coefficients
betaVec <- rbind(c(12,12,0), c(8,8,8), c(4,16,4))
set.seed(20)
for (nInd in seq_along(nList)){
n <- nList[nInd]
for (sigInd in seq_along(sigList)){
sig <- sigList[sigInd]
for (betasetting in c(1:3)){
# generate the parameters for betas
be1 <- rep(1,betaVec[betasetting,1])
be2 <- rep(0,betaVec[betasetting,2])
be3 <- rep(1,betaVec[betasetting,3])
TrainingRes <- TestFun(nrep=nrep,
n=n,
Xdist=Xdist,
nPred=nPred,
sig=sig,
nit=nit,
nK=nK,
be1=be1,
be2=be2,
be3=be3)
filename <- paste("_betasetting_", betasetting, "n_", n, "_", "sig_", sig, "_Xdist_",  Xdist,  sep="")
write.csv(TrainingRes$MSE, paste("data/ridgeTrainingAssistedMSE_", filename, ".csv", sep=""))
write.csv(TrainingRes$MSE_oracle, paste("data/ridgeTrainingOracleMSE_", filename, ".csv", sep=""))
write.csv(TrainingRes$lambdaAssVec, paste("data/ridgeTrainingAssistedLam_", filename, ".csv", sep=""))
write.csv(TrainingRes$lambdaOraVec, paste("data/ridgeTrainingOracleLam_", filename, ".csv", sep=""))
}
}
}
test <- c(1:10)
test[-c(1:5)]
exp(9*2.8)
exp(1/0.125)
library(glmnet)
?cv.glmnet
197*2 + 1
0.7^6 + 0.3 ^ 6
0.95^6 + 0.05^6
